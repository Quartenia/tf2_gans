{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to tf2_gans.dcgan,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "from nbdev import *\n",
    "%nbdev_default_export dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf2_gans.losses import BCE_generator_loss, BCE_discriminator_loss\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    def __init__(self, noise_dim, out_channels=1):\n",
    "        self.noise_dim = noise_dim\n",
    "        self.generator = Generator(\n",
    "            noise_dim=noise_dim, out_channels=out_channels)\n",
    "        self.discriminator = Discriminator()\n",
    "        self.generator_loss = BCE_generator_loss\n",
    "        self.discriminator_loss = BCE_discriminator_loss\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4,\n",
    "                                                            beta_1=0.5)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4,\n",
    "                                                                beta_1=0.5)\n",
    "\n",
    "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=self.generator_optimizer,\n",
    "                                              generator=self.generator,\n",
    "                                              discriminator_optimizer=self.discriminator_optimizer,\n",
    "                                              discriminator=self.discriminator)\n",
    "\n",
    "    def save(self, file_prefix):\n",
    "        self.checkpoint.save(file_prefix)\n",
    "\n",
    "    def generate(self, input_image):\n",
    "        return self.generator(input_image, training=True)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, noise, target):\n",
    "        with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
    "            generated_images = self.generator(noise, training=True)\n",
    "\n",
    "            real_output = self.discriminator(target, training=True)\n",
    "            fake_output = self.discriminator(generated_images, training=True)\n",
    "\n",
    "            gen_loss = self.generator_loss(fake_output)\n",
    "            disc_loss = self.discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                                     self.discriminator.trainable_variables)\n",
    "        self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                                         self.discriminator.trainable_variables))\n",
    "\n",
    "        generator_gradients = gen_tape.gradient(gen_loss,\n",
    "                                                self.generator.trainable_variables)\n",
    "\n",
    "        self.generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                                     self.generator.trainable_variables))\n",
    "\n",
    "        return {'gen_total_loss': gen_loss,\n",
    "                'disc_loss': disc_loss}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self):\n",
    "        self.has_trained = False\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def set_training_parameters(self,\n",
    "                                epochs=10,\n",
    "                                continue_training=False,\n",
    "                                save_fq=0,\n",
    "                                preview_fq=5,\n",
    "                                progress_fn=None,\n",
    "                                preview_fn=None):\n",
    "        self.epochs = epochs\n",
    "        self.continue_training = continue_training\n",
    "        self.save_fq = save_fq\n",
    "        self.preview_fq = preview_fq\n",
    "        self.progress_fn = progress_fn\n",
    "        self.preview_fn = preview_fn\n",
    "\n",
    "        print(\"parameters set\")\n",
    "\n",
    "    def train_epoch(self):\n",
    "        pass\n",
    "\n",
    "    def preprocess_data(self, raw_data):\n",
    "        self.raw_data = raw_data\n",
    "        return self.raw_data\n",
    "\n",
    "    def dataset_generator(self):\n",
    "        pass\n",
    "\n",
    "    def create_tf_dataset(self):\n",
    "        self.dataset = tf.data.Dataset.from_generator(self.dataset_generator,\n",
    "                                                      self.output_type)\n",
    "        self.dataset = self.dataset.batch(self.batch_size)\n",
    "        return self.dataset\n",
    "\n",
    "    def init_dataset(self, dataset, batch_size=1, buffer_size=100):\n",
    "        # Batch and shuffle the data\n",
    "        self.dataset = dataset.shuffle(buffer_size)\n",
    "        self.dataset = self.dataset.batch(batch_size)\n",
    "        return self.dataset\n",
    "\n",
    "    def train(self):\n",
    "        if self.progress_fn:\n",
    "            self.progress_fn(0.0)\n",
    "        self.has_trained = True\n",
    "\n",
    "        if self.continue_training:\n",
    "            pass\n",
    "        else:\n",
    "            # reset weights\n",
    "            pass\n",
    "\n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            self.train_epoch()\n",
    "\n",
    "            self.end_epoch(epoch)\n",
    "\n",
    "        print(\"saving last epoch\")\n",
    "\n",
    "    def end_epoch(self, epoch):\n",
    "        if self.progress_fn:\n",
    "            self.progress_fn((100*epoch)//self.epochs)\n",
    "\n",
    "        if epoch % (self.epochs//self.preview_fq) == 0:\n",
    "            if self.preview_fn:\n",
    "                self.preview_fn(self.generate_preview())\n",
    "\n",
    "        print(\"Epoch: \", epoch)\n",
    "\n",
    "        # if (self.epochs//self.save_fq) == 0:\n",
    "        #     self.gan.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    def generate_preview(self):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(keras.Model):\n",
    "    def __init__(self,\n",
    "                 conv_dim=64,\n",
    "                 n_down_blocks=2,\n",
    "                 dropout_rate=0.3,\n",
    "                 leaky_alpha=0.2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = keras.Sequential(name=\"main\")\n",
    "        for n in range(n_down_blocks):\n",
    "            self.main.add(keras.layers.Conv2D(conv_dim*(2**n), 4,\n",
    "                                              strides=2,\n",
    "                                              padding='same',\n",
    "                                              use_bias=False))\n",
    "            self.main.add(keras.layers.BatchNormalization())\n",
    "            self.main.add(keras.layers.LeakyReLU(leaky_alpha))\n",
    "            self.main.add(keras.layers.Dropout(dropout_rate))\n",
    "        self.main.add(keras.layers.Flatten())\n",
    "        self.main.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.main(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "    def __init__(self,\n",
    "                 output_size=28,\n",
    "                 conv_dim=64,\n",
    "                 noise_dim=100,\n",
    "                 n_up_blocks=1,\n",
    "                 c_dim=5,\n",
    "                 out_channels=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = keras.Sequential(name=\"main\")\n",
    "        init_size = output_size // (2**(n_up_blocks+1))\n",
    "        conv_dim = conv_dim*(2**(n_up_blocks+1))\n",
    "        self.main.add(keras.layers.Dense(init_size*init_size *\n",
    "                                         noise_dim, use_bias=False, input_shape=(noise_dim,)))\n",
    "        self.main.add(keras.layers.Reshape((init_size, init_size, )))\n",
    "        self.main.add(keras.layers.Conv2DTranspose(conv_dim, 4,\n",
    "                                                   strides=(1, 1),\n",
    "                                                   padding='valid',\n",
    "                                                   use_bias=False))\n",
    "        self.main.add(keras.layers.BatchNormalization())\n",
    "        self.main.add(keras.layers.LeakyReLU())\n",
    "\n",
    "        curr_dim = conv_dim // 2\n",
    "\n",
    "        for n in range(n_up_blocks):\n",
    "            self.main.add(keras.layers.Conv2DTranspose(curr_dim,\n",
    "                                                       4,\n",
    "                                                       strides=(2, 2),\n",
    "                                                       padding='same',\n",
    "                                                       use_bias=False))\n",
    "            self.main.add(keras.layers.BatchNormalization())\n",
    "            self.main.add(keras.layers.LeakyReLU())\n",
    "            curr_dim = curr_dim // 2\n",
    "\n",
    "        self.main.add(keras.layers.Conv2DTranspose(out_channels,\n",
    "                                                   (5, 5),\n",
    "                                                   strides=(2, 2),\n",
    "                                                   padding='same',\n",
    "                                                   use_bias=False,\n",
    "                                                   activation='tanh'))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.main(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_dcgan.ipynb.\n",
      "Converted blocks.ipynb.\n",
      "Converted core.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted losses.ipynb.\n"
     ]
    }
   ],
   "source": [
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
